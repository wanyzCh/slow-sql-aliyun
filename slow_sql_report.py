# slow_sql_report.py

import json
import hashlib
import datetime
import requests
import sys
from collections import defaultdict
try:
    from aliyunsdkcore.client import AcsClient
    from aliyunsdkrds.request.v20140815.DescribeSlowLogRecordsRequest import DescribeSlowLogRecordsRequest
    from config import ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID, DB_INSTANCE_ID, FEISHU_WEBHOOK
except ImportError as e:
    print(f"[ERROR] å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    sys.exit(1)

# === æ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯éƒ¨åˆ†éšè—ï¼‰===
print(f"[DEBUG] åŒºåŸŸ: {REGION_ID}")
print(f"[DEBUG] å®ä¾‹ID: {DB_INSTANCE_ID}")
print(f"[DEBUG] ACCESS_KEY_ID: {ACCESS_KEY_ID[:4]}{'*' * (len(ACCESS_KEY_ID) - 8)}{ACCESS_KEY_ID[-4:]}")
print(f"[DEBUG] FEISHU_WEBHOOK: {'å·²é…ç½®' if FEISHU_WEBHOOK and FEISHU_WEBHOOK != 'YOUR_FEISHU_WEBHOOK_URL' else 'æœªé…ç½®'}")

# === è·å–ä¸Šå‘¨æ—¶é—´èŒƒå›´ ===
end_time = datetime.datetime.now()
start_time = end_time - datetime.timedelta(days=7)
start_str = start_time.strftime("%Y-%m-%dT00:00Z")
end_str = end_time.strftime("%Y-%m-%dT00:00Z")

# === åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯ ===
try:
    client = AcsClient(ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID)
    print("[INFO] æˆåŠŸåˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯")
except Exception as e:
    print(f"[ERROR] åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯å¤±è´¥: {e}")
    sys.exit(1)

# === æ„å»ºè¯·æ±‚ ===
request = DescribeSlowLogRecordsRequest()
request.set_DBInstanceId(DB_INSTANCE_ID)
request.set_StartTime(start_str)
request.set_EndTime(end_str)
request.set_accept_format('json')
# è®¾ç½®æ¯é¡µè®°å½•æ•°é‡
request.set_PageSize(100)

print(f"[INFO] æŸ¥è¯¢æ—¶é—´èŒƒå›´: {start_str} ~ {end_str}")

# === å‘é€è¯·æ±‚å¹¶è·å–æ‰€æœ‰è®°å½•ï¼ˆåˆ†é¡µå¤„ç†ï¼‰ ===
all_slow_logs = []
page_number = 1
max_pages = 50  # æœ€å¤šè·å–20é¡µï¼Œå¯¹åº”5000æ¡è®°å½•
total_records = 0

try:
    # åˆ†é¡µæŸ¥è¯¢æ‰€æœ‰æ…¢æŸ¥è¯¢è®°å½•
    while page_number <= max_pages:
        request.set_PageNumber(page_number)
        response = client.do_action_with_exception(request)
        print(f"[INFO] æˆåŠŸå‘é€ç¬¬{page_number}é¡µè¯·æ±‚è‡³é˜¿é‡Œäº‘")
        result = json.loads(response)
        
        # è·å–é¡µé¢ä¿¡æ¯
        total_records = result.get('TotalRecordCount', 0)
        page_records = result.get('PageRecordCount', 0)
        
        print(f"[INFO] æ€»è®°å½•æ•°: {total_records}, å½“å‰é¡µè®°å½•æ•°: {page_records}, å½“å‰é¡µç : {page_number}")
        
        # è·å–æ…¢æŸ¥è¯¢è®°å½•
        items = result.get("Items", {})
        page_slow_logs = items.get("SQLSlowRecord", [])
        
        if not page_slow_logs:
            print(f"[INFO] ç¬¬{page_number}é¡µæ²¡æœ‰æ‰¾åˆ°æ…¢æŸ¥è¯¢è®°å½•")
            break
        
        all_slow_logs.extend(page_slow_logs)
        print(f"[INFO] å·²ç´¯è®¡è·å– {len(all_slow_logs)} æ¡æ…¢æŸ¥è¯¢è®°å½•")
        
        # å¦‚æœå½“å‰é¡µè®°å½•æ•°å°äºé¡µå¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
        if page_records < 100:
            break
            
        page_number += 1
    
    if not all_slow_logs:
        print("[WARN] æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ…¢æŸ¥è¯¢è®°å½•")
        sys.exit(0)
    
    print(f"[INFO] å…±è·å–åˆ° {len(all_slow_logs)} æ¡æ…¢æŸ¥è¯¢è®°å½•ï¼Œå¼€å§‹åˆ†æ...")
    
except Exception as e:
    print(f"[ERROR] è°ƒç”¨é˜¿é‡Œäº‘APIå¤±è´¥: {e}")
    sys.exit(1)

# === æ•°æ®èšåˆ ===
summary = defaultdict(lambda: {"count": 0, "total_time": 0.0, "max_time": 0.0, "total_scanned_rows": 0, "total_parse_rows": 0})

excluded_users = ["risk_dw_bin_ro"]  # è¦æ’é™¤çš„ç”¨æˆ·åˆ—è¡¨
excluded_count = 0

for record in all_slow_logs:
    sql = record.get("SQLText", "").strip()
    if not sql:
        continue
    
    # è·å–ç”¨æˆ·åä¿¡æ¯
    username = record.get("AccountName", "")
    # æ’é™¤æŒ‡å®šç”¨æˆ·çš„æ…¢SQL
    if username in excluded_users:
        excluded_count += 1
        continue
        
    # ä½¿ç”¨SQLHashä½œä¸ºé”®ï¼Œè¿™æ ·æ›´å‡†ç¡®
    key = record.get("SQLHash", hashlib.md5(sql.encode()).hexdigest()[:10])
    
    # æŸ¥è¯¢æ—¶é—´ï¼Œå•ä½æ¯«ç§’
    query_time = float(record.get("QueryTimeMS", 0))
    
    # æ‰«æè¡Œæ•° - ä»ScanRowså­—æ®µæˆ–æ–°çš„å­—æ®µè·å–
    scanned_rows = int(record.get("ScanRows", 0))
    if scanned_rows == 0:  # å¦‚æœScanRowsä¸º0ï¼Œå°è¯•ä½¿ç”¨ReturnRowCounts
        scanned_rows = int(record.get("ReturnRowCounts", 0))
    
    # è§£æè¡Œæ•° - æ·»åŠ ParseRowCountså­—æ®µ
    parse_rows = int(record.get("ParseRowCounts", 0))
    
    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©æŸ¥çœ‹åŸå§‹æ•°æ®
    if page_number <= 1 and len(all_slow_logs) < 10:
        print(f"[DEBUG] SQL: {sql[:50]}...")
        print(f"[DEBUG] ScanRows: {record.get('ScanRows', 'N/A')}, ReturnRowCounts: {record.get('ReturnRowCounts', 'N/A')}, ParseRowCounts: {record.get('ParseRowCounts', 'N/A')}")
    
    # æ›´æ–°æˆ–åˆå§‹åŒ–è®°å½•
    if key not in summary:
        summary[key]["sql"] = sql  # ä¿å­˜å®Œæ•´SQLï¼Œä¸å†æˆªæ–­
        
    summary[key]["count"] += int(record.get("QueryTimes", 1))
    summary[key]["total_time"] += query_time
    summary[key]["db_name"] = record.get("DBName", "")
    summary[key]["max_time"] = max(summary[key]["max_time"], query_time)
    summary[key]["host_address"] = record.get("HostAddress", "")
    summary[key]["username"] = username  # ä¿å­˜ç”¨æˆ·åä¿¡æ¯
    summary[key]["total_scanned_rows"] += scanned_rows
    summary[key]["total_parse_rows"] += parse_rows

print(f"[INFO] å·²æ’é™¤ {excluded_count} æ¡æ¥è‡ª {', '.join(excluded_users)} ç”¨æˆ·çš„è®°å½•")

# === è®¡ç®—ç»¼åˆè¯„åˆ† ===
for key, data in summary.items():
    avg_time = data["total_time"] / data["count"]
    # ç»¼åˆè¯„åˆ† = å¹³å‡æ‰§è¡Œæ—¶é—´ Ã— æ‰§è¡Œæ¬¡æ•° Ã— log(æ‰«æè¡Œæ•°+1)
    data["score"] = avg_time * data["count"] * max(1, (data["total_scanned_rows"] / data["count"]) ** 0.5 / 10)

# === æ’åºå¹¶ç”Ÿæˆ Markdown è¡¨æ ¼ ===
top_slow_sql = sorted(summary.values(), key=lambda x: x["score"], reverse=True)[:200]
print(f"[INFO] ç”Ÿæˆäº† {len(top_slow_sql)} æ¡èšåˆçš„æ…¢æŸ¥è¯¢æ•°æ®")

# ä¸ºé£ä¹¦å‡†å¤‡è¡¨æ ¼å†…å®¹
table_content = []
for item in top_slow_sql:
    avg = round(item["total_time"] / item["count"], 2)
    max_time = round(item["max_time"], 2)
    avg_rows = round(item["total_scanned_rows"] / item["count"]) if item["count"] > 0 else 0
    avg_parse_rows = round(item["total_parse_rows"] / item["count"]) if item["count"] > 0 else 0
    table_content.append([
        item['sql'], 
        item['db_name'], 
        item['host_address'], 
        item.get('username', 'æœªçŸ¥'),
        str(item['count']), 
        str(avg), 
        str(max_time),
        str(avg_rows),
        str(avg_parse_rows)
    ])

# === æ¨é€åˆ°é£ä¹¦ç¾¤ ===
if len(top_slow_sql) > 0 and FEISHU_WEBHOOK and FEISHU_WEBHOOK != "YOUR_FEISHU_WEBHOOK_URL":
    # ä½¿ç”¨å¡ç‰‡æ¶ˆæ¯æ ¼å¼ï¼Œæé«˜å¯è¯»æ€§
    card = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "title": {
                    "tag": "plain_text",
                    "content": f"ğŸ¢ æœ¬å‘¨æ…¢ SQL æŠ¥å‘Šï¼ˆ{start_time.date()} ~ {end_time.date()}ï¼‰"
                },
                "template": "red"
            },
            "elements": [
                {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ€»å…±å‘ç° {total_records} æ¡æ…¢æŸ¥è¯¢è®°å½•ï¼Œåˆ†æäº† {len(all_slow_logs)} æ¡ï¼ˆæ’é™¤äº† {excluded_count} æ¡ {', '.join(excluded_users)} ç”¨æˆ·çš„è®°å½•ï¼‰ï¼Œä»¥ä¸‹æ˜¯æœ€éœ€è¦ä¼˜åŒ–çš„å‰200æ¡:**"
                    }
                },
                {
                    "tag": "hr"
                }
            ]
        }
    }
    
    # æ·»åŠ æ¯æ¡æ…¢æŸ¥è¯¢çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»…å±•ç¤ºå‰20æ¡è¯¦æƒ…ï¼Œå…¶ä½™ä»¥è¡¨æ ¼å½¢å¼å±•ç¤ºï¼‰
    for i, item in enumerate(top_slow_sql[:20]):
        avg_time = round(item["total_time"] / item["count"], 2)
        max_time = round(item["max_time"], 2)
        # ç¡®ä¿æ­£ç¡®è§£æè¡Œæ•°
        avg_rows = round(item["total_scanned_rows"] / item["count"]) if item["count"] > 0 else 0
        avg_parse_rows = round(item["total_parse_rows"] / item["count"]) if item["count"] > 0 else 0
        
        # æˆªæ–­è¿‡é•¿çš„SQLï¼Œä½¿æ¶ˆæ¯æ›´ç¾è§‚
        sql_display = item['sql']
        if len(sql_display) > 200:
            sql_display = sql_display[:197] + "..."
        
        # ç¾åŒ–æ˜¾ç¤º
        card["card"]["elements"].append({
            "tag": "div",
            "fields": [
                {
                    "is_short": False,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**#{i+1} SQL:** `{sql_display}`"
                    }
                }
            ]
        })
        
        # æ·»åŠ è¯¦ç»†ä¿¡æ¯è¡¨æ ¼
        card["card"]["elements"].append({
            "tag": "div",
            "fields": [
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ•°æ®åº“:** {item['db_name']}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**ä¸»æœº:** {item['host_address']}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**è´¦å·:** {item.get('username', 'æœªçŸ¥')}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ‰§è¡Œæ¬¡æ•°:** {item['count']}"
                    }
                }
            ]
        })
        
        # æ€§èƒ½æŒ‡æ ‡
        card["card"]["elements"].append({
            "tag": "div",
            "fields": [
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**å¹³å‡è€—æ—¶:** {avg_time}ms"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æœ€å¤§è€—æ—¶:** {max_time}ms"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**å¹³å‡æ‰«æè¡Œæ•°:** {avg_rows}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**å¹³å‡è§£æè¡Œæ•°:** {avg_parse_rows}"
                    }
                }
            ]
        })
        
        # æ·»åŠ åˆ†éš”çº¿
        if i < len(top_slow_sql[:20]) - 1:
            card["card"]["elements"].append({
                "tag": "hr"
            })
    
    # å¦‚æœæœ‰è¶…è¿‡20æ¡è®°å½•ï¼Œå°†å‰©ä½™è®°å½•ä»¥ç®€æ´è¡¨æ ¼å½¢å¼æ·»åŠ 
    if len(top_slow_sql) > 20:
        table_rows = []
        for i, item in enumerate(top_slow_sql[20:200], 21):
            avg_time = round(item["total_time"] / item["count"], 2)
            max_time = round(item["max_time"], 2)
            avg_rows = round(item["total_scanned_rows"] / item["count"]) if item["count"] > 0 else 0
            avg_parse_rows = round(item["total_parse_rows"] / item["count"]) if item["count"] > 0 else 0
            
            # è¡¨æ ¼ä¸­SQLè¿˜æ˜¯éœ€è¦é™åˆ¶é•¿åº¦ï¼Œå¦åˆ™ä¼šå½±å“å¯è¯»æ€§
            sql_preview = item['sql']
            if len(sql_preview) > 80:
                sql_preview = sql_preview[:77] + "..."
            
            username = item.get('username', 'æœªçŸ¥')
            
            table_rows.append(f"| {i} | {sql_preview} | {item['db_name']} | {username} | {item['count']} | {avg_time} | {avg_rows} | {avg_parse_rows} |")
        
        table_header = "| åºå· | SQL | æ•°æ®åº“ | è´¦å· | æ‰§è¡Œæ¬¡æ•° | å¹³å‡è€—æ—¶(ms) | å¹³å‡æ‰«æè¡Œæ•° | å¹³å‡è§£æè¡Œæ•° |\n|------|-----|--------|------|---------|------------|------------|------------|\n"
        table_content = table_header + "\n".join(table_rows)
        
        card["card"]["elements"].append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": "**å‰©ä½™éœ€ä¼˜åŒ–çš„SQLæŸ¥è¯¢:**"
            }
        })
        
        # å°†è¡¨æ ¼åˆ†æ®µå‘é€ï¼Œé¿å…å†…å®¹è¿‡é•¿
        table_chunks = [table_rows[i:i+30] for i in range(0, len(table_rows), 30)]
        for chunk_idx, chunk in enumerate(table_chunks):
            chunk_content = table_header + "\n".join(chunk)
            card["card"]["elements"].append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"**è®°å½• {20+chunk_idx*30+1}-{min(20+(chunk_idx+1)*30, len(top_slow_sql))}:**\n{chunk_content}"
                }
            })
    
    try:
        print("[INFO] å‘é€å¡ç‰‡æ¶ˆæ¯åˆ°é£ä¹¦...")
        resp = requests.post(FEISHU_WEBHOOK, data=json.dumps(card), headers={"Content-Type": "application/json"})
        print(f"[INFO] æ¨é€çŠ¶æ€: {resp.status_code}, è¿”å›: {resp.text}")
        
        # å¦‚æœå¡ç‰‡æ¶ˆæ¯å¤±è´¥ï¼Œå°è¯•å‘é€ç®€å•æ–‡æœ¬æ¶ˆæ¯
        if resp.status_code != 200:
            print("[WARN] å¡ç‰‡æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œå°è¯•å‘é€ç®€å•æ–‡æœ¬æ¶ˆæ¯...")
            simple_payload = {
                "msg_type": "text",
                "content": {
                    "text": f"ğŸ¢ æœ¬å‘¨æ…¢ SQL æŠ¥å‘Šï¼ˆ{start_time.date()} ~ {end_time.date()}ï¼‰\n\n" +
                            f"æ€»å…±å‘ç° {total_records} æ¡æ…¢æŸ¥è¯¢è®°å½•ï¼Œåˆ†æäº† {len(all_slow_logs)} æ¡ï¼ˆæ’é™¤äº† {excluded_count} æ¡ {', '.join(excluded_users)} ç”¨æˆ·çš„è®°å½•ï¼‰ï¼Œä»¥ä¸‹æ˜¯æœ€éœ€è¦ä¼˜åŒ–çš„å‰20æ¡:\n\n" +
                            "\n".join([
                                f"- **#{i+1}** SQL: {item['sql'][:150]}...\n" +
                                f"  æ•°æ®åº“: {item['db_name']} | ä¸»æœº: {item['host_address']} | è´¦å·: {item.get('username', 'æœªçŸ¥')}\n" +
                                f"  æ‰§è¡Œ: {item['count']}æ¬¡ | å¹³å‡: {avg_time}ms | æœ€å¤§: {max_time}ms\n" +
                                f"  æ‰«æè¡Œ: {avg_rows} | è§£æè¡Œ: {avg_parse_rows}"
                                for i, item in enumerate(top_slow_sql[:20])
                            ]) + 
                            f"\n\næ³¨æ„ï¼šå…±å‘ç° {len(top_slow_sql)} æ¡éœ€è¦ä¼˜åŒ–çš„SQLï¼Œæ­¤å¤„ä»…å±•ç¤ºå‰20æ¡ã€‚"
                }
            }
            resp = requests.post(FEISHU_WEBHOOK, data=json.dumps(simple_payload), headers={"Content-Type": "application/json"})
            print(f"[INFO] ç®€å•æ¶ˆæ¯æ¨é€çŠ¶æ€: {resp.status_code}, è¿”å›: {resp.text}")
    except Exception as e:
        print(f"[ERROR] æ¨é€åˆ°é£ä¹¦å¤±è´¥: {e}")
else:
    if len(top_slow_sql) == 0:
        print("[INFO] æ²¡æœ‰æ…¢æŸ¥è¯¢æ•°æ®ï¼Œè·³è¿‡æ¨é€")
    elif FEISHU_WEBHOOK == "YOUR_FEISHU_WEBHOOK_URL":
        print("[WARN] é£ä¹¦ Webhook URL æœªé…ç½®ï¼Œè·³è¿‡æ¨é€")
    elif not FEISHU_WEBHOOK:
        print("[WARN] é£ä¹¦ Webhook URL ä¸ºç©ºï¼Œè·³è¿‡æ¨é€")

# è¾“å‡ºç»“æœåˆ°æ§åˆ¶å°
print("\n===== æ…¢æŸ¥è¯¢æŠ¥å‘Š =====")
print(f"æ—¶é—´èŒƒå›´: {start_time.date()} ~ {end_time.date()}")
print(f"æ€»è®°å½•æ•°: {total_records}, åˆ†æè®°å½•æ•°: {len(all_slow_logs)}")
print(f"å·²æ’é™¤ {excluded_count} æ¡æ¥è‡ª {', '.join(excluded_users)} ç”¨æˆ·çš„è®°å½•")
print("Top 200 æ…¢æŸ¥è¯¢:")
print("| åºå· | SQL | æ•°æ®åº“ | ä¸»æœº | è´¦å· | æ¬¡æ•° | å¹³å‡è€—æ—¶(ms) | æœ€å¤§è€—æ—¶(ms) | å¹³å‡æ‰«æè¡Œæ•° |")
print("|------|-----|--------|------|------|------|--------------|--------------|------------|")
for i, item in enumerate(top_slow_sql):
    avg = round(item["total_time"] / item["count"], 2)
    max_time = round(item["max_time"], 2)
    avg_rows = round(item["total_scanned_rows"] / item["count"]) if item["count"] > 0 else 0
    
    # æ§åˆ¶å°è¾“å‡ºæ—¶SQLä»éœ€è¦é™åˆ¶é•¿åº¦ï¼Œä»¥ä¾¿æ‰“å°
    sql_preview = item['sql']
    if len(sql_preview) > 100:
        sql_preview = sql_preview[:97] + "..."
        
    username = item.get('username', 'æœªçŸ¥')
    print(f"| {i+1} | {sql_preview} | {item['db_name']} | {item['host_address']} | {username} | {item['count']} | {avg} | {max_time} | {avg_rows} |")