#!/usr/bin/env python3
# slow_sql_statistics.py
# ä½¿ç”¨ DescribeSlowLogs API æŸ¥è¯¢ RDS å®ä¾‹çš„æ…¢æ—¥å¿—ç»Ÿè®¡æƒ…å†µ

import json
import datetime
import requests
import sys
from collections import defaultdict

try:
    from aliyunsdkcore.client import AcsClient
    from aliyunsdkrds.request.v20140815.DescribeSlowLogsRequest import DescribeSlowLogsRequest
    from config import ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID, DB_INSTANCE_ID, FEISHU_WEBHOOK
except ImportError as e:
    print(f"[ERROR] å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    print("[INFO] è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install aliyun-python-sdk-core aliyun-python-sdk-rds requests")
    sys.exit(1)

# === æ˜¾ç¤ºé…ç½®ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯éƒ¨åˆ†éšè—ï¼‰===
print(f"[DEBUG] åŒºåŸŸ: {REGION_ID}")
print(f"[DEBUG] å®ä¾‹ID: {DB_INSTANCE_ID}")
print(f"[DEBUG] ACCESS_KEY_ID: {ACCESS_KEY_ID[:4]}{'*' * (len(ACCESS_KEY_ID) - 8)}{ACCESS_KEY_ID[-4:]}")
print(f"[DEBUG] FEISHU_WEBHOOK: {'å·²é…ç½®' if FEISHU_WEBHOOK and FEISHU_WEBHOOK != 'YOUR_FEISHU_WEBHOOK_URL' else 'æœªé…ç½®'}")

# === è·å–æŸ¥è¯¢æ—¶é—´èŒƒå›´ ===
end_time = datetime.datetime.now()
start_time = end_time - datetime.timedelta(days=7)  # é»˜è®¤æŸ¥è¯¢æœ€è¿‘7å¤©
start_str = start_time.strftime("%Y-%m-%dZ")  # UTC æ ¼å¼ï¼Œä¸ API è¦æ±‚ä¿æŒä¸€è‡´
end_str = end_time.strftime("%Y-%m-%dZ")      # UTC æ ¼å¼ï¼Œä¸ API è¦æ±‚ä¿æŒä¸€è‡´

# === åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯ ===
try:
    client = AcsClient(ACCESS_KEY_ID, ACCESS_KEY_SECRET, REGION_ID)
    print("[INFO] æˆåŠŸåˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯")
except Exception as e:
    print(f"[ERROR] åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯å¤±è´¥: {e}")
    sys.exit(1)

# === æ„å»ºè¯·æ±‚ ===
request = DescribeSlowLogsRequest()
request.set_DBInstanceId(DB_INSTANCE_ID)
request.set_StartTime(start_str)
request.set_EndTime(end_str)
# å¯é€‰å‚æ•°ï¼šè®¾ç½®æ’åºé”®
request.set_SortKey("TotalExecutionCounts")  # æŒ‰æ€»æ‰§è¡Œæ¬¡æ•°æ’åº
# å¯é€‰å‚æ•°ï¼šè®¾ç½®æ•°æ®åº“å
# request.set_DBName("your_db_name")
request.set_PageSize(100)  # æ¯é¡µè¿”å›çš„è®°å½•æ•°
request.set_accept_format('json')

print(f"[INFO] æŸ¥è¯¢æ—¶é—´èŒƒå›´: {start_str} ~ {end_str}")

# === å‘é€è¯·æ±‚å¹¶è·å–æ‰€æœ‰è®°å½•ï¼ˆåˆ†é¡µå¤„ç†ï¼‰ ===
all_slow_logs = []
page_number = 1
max_pages = 20  # æœ€å¤šè·å–20é¡µæ•°æ®
total_records = 0

try:
    # åˆ†é¡µæŸ¥è¯¢æ‰€æœ‰æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•
    while page_number <= max_pages:
        request.set_PageNumber(page_number)
        response = client.do_action_with_exception(request)
        print(f"[INFO] æˆåŠŸå‘é€ç¬¬{page_number}é¡µè¯·æ±‚è‡³é˜¿é‡Œäº‘")
        result = json.loads(response)
        # è·å–é¡µé¢ä¿¡æ¯
        total_records = result.get('TotalRecordCount', 0)
        items = result.get("Items", {})
        page_slow_logs = items.get("SQLSlowLog", [])
        
        if not page_slow_logs:
            print(f"[INFO] ç¬¬{page_number}é¡µæ²¡æœ‰æ‰¾åˆ°æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•")
            break
        
        all_slow_logs.extend(page_slow_logs)
        print(f"[INFO] å·²ç´¯è®¡è·å– {len(all_slow_logs)} æ¡æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•")
        
        # å¦‚æœå½“å‰é¡µè®°å½•æ•°å°äºé¡µå¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
        if len(page_slow_logs) < 100:
            break
            
        page_number += 1
    
    if not all_slow_logs:
        print("[WARN] æ²¡æœ‰æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•")
        sys.exit(0)
    
    print(f"[INFO] å…±è·å–åˆ° {len(all_slow_logs)} æ¡æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•")
    
except Exception as e:
    print(f"[ERROR] è°ƒç”¨é˜¿é‡Œäº‘ DescribeSlowLogs API å¤±è´¥: {e}")
    sys.exit(1)

# === å¤„ç†å¹¶æ˜¾ç¤ºç»“æœ ===
# æŒ‰SQLæ¨¡æ¿çš„æ‰§è¡Œæ¬¡æ•°æ’åº
sorted_slow_logs = sorted(all_slow_logs, key=lambda x: int(x.get('MySQLTotalExecutionCounts', 0)), reverse=True)

# ç”Ÿæˆ Markdown è¡¨æ ¼å†…å®¹
markdown_table = "### æ…¢æŸ¥è¯¢ç»Ÿè®¡æŠ¥å‘Š\n\n"
markdown_table += f"**æŸ¥è¯¢æ—¶é—´èŒƒå›´**: {start_time.strftime('%Y-%m-%d')} è‡³ {end_time.strftime('%Y-%m-%d')}\n\n"
markdown_table += "| # | æ•°æ®åº“ | SQLæ¨¡æ¿ | æ‰§è¡Œæ¬¡æ•° | å¹³å‡æ‰§è¡Œæ—¶é—´(ms) | æœ€å¤§æ‰§è¡Œæ—¶é—´(ms) | è§£æè¡Œæ•°(æ€»è®¡) | æ‰«æè¡Œæ•°(æœ€å¤§) |\n"
markdown_table += "|---|--------|---------|----------|----------------|----------------|--------------|----------------|\n"

# ä¸ºé£ä¹¦å‡†å¤‡è¡¨æ ¼å†…å®¹
table_content = []

for i, item in enumerate(sorted_slow_logs[:50]):  # åªæ˜¾ç¤ºå‰50æ¡
    db_name = item.get('DBName', 'N/A')
    
    # è·å–SQLæ¨¡æ¿ï¼Œæ³¨æ„è¿™ä¸ªAPIè¿”å›çš„æ˜¯SQLæ¨¡æ¿
    sql_template = item.get('SQLText', 'N/A')
    if len(sql_template) > 300:  # å¢åŠ æ˜¾ç¤ºé•¿åº¦ä»100åˆ°300
        sql_template = sql_template[:297] + "..."
    
    # ç»Ÿè®¡æ•°æ®
    total_count = item.get('MySQLTotalExecutionCounts', 0)
    total_time = float(item.get('MySQLTotalExecutionTimes', 0))
    avg_time = round(total_time / total_count if total_count > 0 else 0, 2)
    max_time = item.get('MaxExecutionTimeMS', 0)
    parse_rows = item.get('ParseTotalRowCounts', 0)
    max_scan_rows = item.get('ParseMaxRowCount', 0)
    
    # æ·»åŠ åˆ°Markdownè¡¨æ ¼
    markdown_table += f"| {i+1} | {db_name} | `{sql_template}` | {total_count} | {avg_time} | {max_time} | {parse_rows} | {max_scan_rows} |\n"
    
    # æ·»åŠ åˆ°é£ä¹¦è¡¨æ ¼å†…å®¹
    table_content.append([
        f"{i+1}",
        db_name,
        sql_template,
        str(total_count),
        str(avg_time),
        str(max_time),
        str(parse_rows),
        str(max_scan_rows)
    ])

print(markdown_table)

# === æ¨é€åˆ°é£ä¹¦ç¾¤ ===
if len(all_slow_logs) > 0 and FEISHU_WEBHOOK and FEISHU_WEBHOOK != "YOUR_FEISHU_WEBHOOK_URL":
    # ä½¿ç”¨å¡ç‰‡æ¶ˆæ¯æ ¼å¼
    card = {
        "msg_type": "interactive",
        "card": {
            "config": {
                "wide_screen_mode": True
            },
            "header": {
                "title": {
                    "tag": "plain_text",
                    "content": f"ğŸ¢ æ…¢ SQL ç»Ÿè®¡æŠ¥å‘Š ({start_time.strftime('%Y-%m-%d')} ~ {end_time.strftime('%Y-%m-%d')})"
                },
                "template": "orange"
            },
            "elements": [
                {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ€»å…±å‘ç° {len(all_slow_logs)} æ¡æ…¢æŸ¥è¯¢ç»Ÿè®¡è®°å½•ï¼Œä»¥ä¸‹æ˜¯æ‰§è¡Œæ¬¡æ•°æœ€å¤šçš„å‰20æ¡:**"
                    }
                },
                {
                    "tag": "hr"
                }
            ]
        }
    }
    
    # æ·»åŠ è¡¨æ ¼
    for i, item in enumerate(sorted_slow_logs[:20]):  # åªæ˜¾ç¤ºå‰20æ¡
        db_name = item.get('DBName', 'N/A')
        sql_template = item.get('SQLText', 'N/A')
        if len(sql_template) > 500:  # å¢åŠ é£ä¹¦æ¶ˆæ¯ä¸­SQLæ˜¾ç¤ºé•¿åº¦ä»200åˆ°500
            sql_template = sql_template[:497] + "..."
        
        total_count = item.get('MySQLTotalExecutionCounts', 0)
        total_time = float(item.get('MySQLTotalExecutionTimes', 0))
        avg_time = round(total_time / total_count if total_count > 0 else 0, 2)
        max_time = item.get('MaxExecutionTimeMS', 0)
        parse_rows = item.get('ParseTotalRowCounts', 0)
        max_scan_rows = item.get('ParseMaxRowCount', 0)
        create_time = item.get('CreateTime', 'N/A')
        
        card["card"]["elements"].append({
            "tag": "div",
            "fields": [
                {
                    "is_short": False,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**#{i+1} SQLæ¨¡æ¿:** `{sql_template}`"
                    }
                }
            ]
        })
        
        card["card"]["elements"].append({
            "tag": "div",
            "fields": [
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ•°æ®åº“:** {db_name}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**åˆ›å»ºæ—¶é—´:** {create_time}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æ‰§è¡Œæ¬¡æ•°:** {total_count}"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**å¹³å‡æ‰§è¡Œæ—¶é—´:** {avg_time}ms"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**æœ€å¤§æ‰§è¡Œæ—¶é—´:** {max_time}ms"
                    }
                },
                {
                    "is_short": True,
                    "text": {
                        "tag": "lark_md",
                        "content": f"**è§£æè¡Œæ•°(æ€»è®¡):** {parse_rows}"
                    }
                }
            ]
        })
        
        # æ·»åŠ åˆ†éš”çº¿
        if i < 19:  # ä¸åœ¨æœ€åä¸€æ¡åæ·»åŠ åˆ†éš”çº¿
            card["card"]["elements"].append({
                "tag": "hr"
            })
    
    # å‘é€è¯·æ±‚åˆ°é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            headers={"Content-Type": "application/json"},
            data=json.dumps(card)
        )
        if response.status_code == 200:
            result = response.json()
            if result.get("code") == 0:
                print("[INFO] æˆåŠŸå‘é€æ…¢SQLç»Ÿè®¡æŠ¥å‘Šåˆ°é£ä¹¦")
            else:
                print(f"[ERROR] å‘é€åˆ°é£ä¹¦å¤±è´¥ï¼Œé”™è¯¯ç : {result.get('code')}, æ¶ˆæ¯: {result.get('msg')}")
        else:
            print(f"[ERROR] å‘é€åˆ°é£ä¹¦å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        print(f"[ERROR] å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™: {e}")
else:
    print("[INFO] æ²¡æœ‰é…ç½®é£ä¹¦ Webhook æˆ–æ²¡æœ‰æ‰¾åˆ°æ…¢æŸ¥è¯¢è®°å½•ï¼Œè·³è¿‡å‘é€")

print("[INFO] æ…¢æŸ¥è¯¢ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆå®Œæˆ") 